# -*- coding: utf-8 -*-
"""Flower_Classifier_TransferLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u1X7A0SlFVCEwqYjeDGt2t9KfUFP5iKM
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import sklearn as sk
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow import keras
import os
import pathlib
import cv2
import PIL.Image
# %load_ext tensorboard

ds=pathlib.Path(r"C:\Users\Sandy\.keras\datasets\flower_photos")

from keras.preprocessing.image import ImageDataGenerator

train_gen=ImageDataGenerator(rotation_range=30, brightness_range=(0.2, 0.1), zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, rescale=1./255)

test_gen=ImageDataGenerator(rescale=1./255)

RESIZE=(224, 224)

train=train_gen.flow_from_directory(r"C:\Users\Sandy\.keras\datasets\flower_photos\train", target_size=RESIZE,
        color_mode="rgb",class_mode='categorical')

test=test_gen.flow_from_directory(r"C:\Users\Sandy\.keras\datasets\flower_photos\test", target_size=RESIZE,
        color_mode="rgb",class_mode='categorical', shuffle=False)

train.class_indices

test.class_indices

y_train=train.classes

len(y_train)

y_test=test.classes

len(y_test)

train.num_classes

INPUT=train.image_shape
INPUT

tf.__version__

feature_extractor_model = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"

feature_detector_layer = hub.KerasLayer(
    feature_extractor_model, input_shape=(224, 224, 3), trainable=False)

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout

model=Sequential()

model.add(feature_detector_layer)

model.add(Dense(units=5, activation='softmax'))

model.compile('adam','categorical_crossentropy',metrics=['accuracy'])

model.summary()

from keras.callbacks import EarlyStopping, History
import datetime

stop=EarlyStopping('val_loss', mode='min', patience=2, verbose=1) #early stopping

history = History() #history

# log_dir = "C:\\Users\\Sandy\\.keras\datasets\\flower_photos\\logs" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S") #tensorboard
# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.fit(train, epochs=20, callbacks=[stop, history], validation_data=test)



metrics=pd.DataFrame(model.history.history)
metrics[['loss', 'val_loss']].plot()

model.evaluate(test)

y_pred=model.predict(test)



real_pred=[]
for i in range(len(y_pred+1)):
    real_pred.append(np.argmax(y_pred[i]))

real_pred=np.array(real_pred)
real_pred
train.class_indices

label={0:'daisy', 1:'dandelion', 2:'roses', 3:'sunflowers', 4:'tulips'}

test.classes

len(real_pred)

real_pred_lab=[]
for p in real_pred:
    real_pred_lab.append(label[p])

y_pred[0]

from sklearn.metrics import confusion_matrix
confusion_matrix(test.classes, real_pred)

from sklearn.metrics import classification_report

print(classification_report(test.classes, real_pred))



def predict(url, resize, true_labels):
    img_arr=plt.imread(url)
    img=cv2.resize(img_arr, resize)
    plt.axis('off')
    plt.imshow(img_arr)
    img=np.expand_dims(img, axis=0)
    prediction=true_labels[np.argmax(model.predict(img))]
    return f"======> {prediction.upper()} <======"

predict(r"C:\Users\Sandy\Desktop\download_1.jpg", (224, 224), label)

tf.keras.models.save_model(model, r"C:\Users\Sandy\Downloads\Train_Models", overwrite=True, include_optimizer=True, save_format="tf")

model=tf.keras.models.load_model(r"C:\Users\Sandy\Downloads\Train_Models")

model.summary()

